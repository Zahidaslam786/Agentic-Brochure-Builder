{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Create a product that builds a Brochure for a company to be used for prospective clients, investors and potential recruits.\n",
        "\n",
        "### We will be provided a company name and their primary website."
      ],
      "metadata": {
        "id": "JYyv17vYmVeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Setup Ollama Server (Background)\n"
      ],
      "metadata": {
        "id": "vOTa_fEMmWrr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5xHAwYB06jm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb7e895-9b5f-4303-cb3f-605edd31d300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tgz\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Ollama server is running in the background!\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. Start Ollama server in the background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start server and suppress output to keep the notebook clean\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# 3. Wait for the server to initialize\n",
        "time.sleep(10)\n",
        "print(\"Ollama server is running in the background!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 1: Imports and Scraper Functions\n"
      ],
      "metadata": {
        "id": "I9auLzR6m9ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for scraping and AI calls\n",
        "!pip install openai beautifulsoup4 requests\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display, update_display\n",
        "\n",
        "# --- Define Scraper Functions (Replacement for Ed's scraper.py) ---\n",
        "\n",
        "def fetch_website_contents(url):\n",
        "    \"\"\"\n",
        "    Fetches the visible text content from a given URL by removing scripts and styles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove non-content tags\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Extract and clean text\n",
        "        return soup.get_text(separator=\"\\n\", strip=True)\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {e}\"\n",
        "\n",
        "def fetch_website_links(url):\n",
        "    \"\"\"\n",
        "    Finds all relevant hyperlinks on a page and converts relative URLs to absolute ones.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find all 'a' tags with href attributes\n",
        "        links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        # Standardize URLs (handle relative links like /about)\n",
        "        base_url = url.rstrip('/')\n",
        "        full_links = []\n",
        "        for link in links:\n",
        "            if link.startswith('http'):\n",
        "                full_links.append(link)\n",
        "            elif link.startswith('/'):\n",
        "                full_links.append(f\"{base_url}{link}\")\n",
        "\n",
        "        # Return unique links only\n",
        "        return list(set(full_links))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching links: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Cell 1: Scraper functions and libraries are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhYdc-apm-zM",
        "outputId": "b3064194-8700-46cf-9581-799e1cd1f67a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Cell 1: Scraper functions and libraries are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 2: Initialize Model and API Client"
      ],
      "metadata": {
        "id": "A-JW-2c5oYTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Download the specific model for our project\n",
        "# We are using llama3.2:1b because it's fast and lightweight for testing\n",
        "!ollama pull llama3.2:1b\n",
        "\n",
        "# 2. Initialize the OpenAI client to connect to the local Ollama server\n",
        "# Ollama provides an OpenAI-compatible endpoint at localhost:11434\n",
        "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
        "\n",
        "# 3. Define the constant for our model name\n",
        "# Ed used 'gpt-5-nano' but we are using our free local model\n",
        "MODEL = \"llama3.2:1b\"\n",
        "\n",
        "print(f\"Cell 2: Client initialized and model {MODEL} is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl3QxgP7oZl7",
        "outputId": "c53d6543-4bf7-403d-edb5-d3386ec51cdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "Cell 2: Client initialized and model llama3.2:1b is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 3: Testing the Scraper (Raw Links Fetch)"
      ],
      "metadata": {
        "id": "tq3lgD3LpSuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target URL & Company Name for our brochure\n",
        "target_url = \"https://huggingface.co/\"\n",
        "company_name = \"Hugging Face\"\n",
        "\n",
        "# Call our scraper function to get all raw links from the page\n",
        "# This is a standard Python step, no AI is involved yet\n",
        "raw_links = fetch_website_links(target_url)\n",
        "\n",
        "# Display the total count and the first few links to verify the scraper is working\n",
        "print(f\"Total links found on {target_url}: {len(raw_links)}\")\n",
        "print(\"First 15 links found:\")\n",
        "for link in raw_links[:15]:\n",
        "    print(f\"- {link}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPttcMYPpVCi",
        "outputId": "7ac20ee8-fd18-4433-cb84-6a7a6a91de6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total links found on https://huggingface.co/: 63\n",
            "First 15 links found:\n",
            "- https://endpoints.huggingface.co\n",
            "- https://huggingface.co/Intel\n",
            "- https://huggingface.co/brand\n",
            "- https://apply.workable.com/huggingface/\n",
            "- https://huggingface.co/join\n",
            "- https://huggingface.co/huggingface\n",
            "- https://huggingface.co/Lightricks/LTX-2\n",
            "- https://huggingface.co/grammarly\n",
            "- https://huggingface.co/docs/smolagents\n",
            "- https://huggingface.co/docs/huggingface_hub\n",
            "- https://huggingface.co/docs/transformers\n",
            "- https://huggingface.co/docs/accelerate\n",
            "- https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate\n",
            "- https://huggingface.co/docs/trl\n",
            "- https://twitter.com/huggingface\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 4: Defining the Link Selection System Prompt"
      ],
      "metadata": {
        "id": "QSQ1pyrlqRT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system prompt for the link selection step\n",
        "# This uses 'One-shot prompting' by providing a single example of the desired JSON output\n",
        "link_system_prompt = \"\"\"\n",
        "You are provided with a list of links found on a webpage.\n",
        "You are able to decide which of the links would be most relevant to include in a brochure about the company,\n",
        "such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
        "You should respond in JSON as in this example:\n",
        "\n",
        "{\n",
        "    \"links\": [\n",
        "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
        "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
        "    ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dxNx7iG4qTjW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 5: The User Prompt Function"
      ],
      "metadata": {
        "id": "e_HI6BbTq9bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links_user_prompt(url):\n",
        "    \"\"\"\n",
        "    Combines instructions with raw scraped links to create a final user prompt.\n",
        "    \"\"\"\n",
        "    # Start the prompt with specific instructions for the LLM\n",
        "    user_prompt = f\"\"\"\n",
        "Here is the list of links on the website {url} -\n",
        "Please decide which of these are relevant web links for a brochure about the company,\n",
        "respond with the full https URL in JSON format.\n",
        "Do not include Terms of Service, Privacy, or email links.\n",
        "\n",
        "Links (some might be relative links):\n",
        "\n",
        "\"\"\"\n",
        "    # Use our scraper to get the actual list of links from the website\n",
        "    links = fetch_website_links(url)\n",
        "\n",
        "    # Append the list of links to the prompt, each on a new line\n",
        "    user_prompt += \"\\n\".join(links)\n",
        "\n",
        "    return user_prompt\n",
        "\n",
        "# Test the function to see what the final prompt looks like\n",
        "# We are still not calling the AI yet; just building the 'Question'\n",
        "test_prompt = get_links_user_prompt(\"https://huggingface.co/\")\n",
        "print(test_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W08TfMkvq-wT",
        "outputId": "ea44a780-a393-4a92-d80a-2247c789d9fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Here is the list of links on the website https://huggingface.co/ -\n",
            "Please decide which of these are relevant web links for a brochure about the company, \n",
            "respond with the full https URL in JSON format.\n",
            "Do not include Terms of Service, Privacy, or email links.\n",
            "\n",
            "Links (some might be relative links):\n",
            "\n",
            "https://endpoints.huggingface.co\n",
            "https://huggingface.co/Intel\n",
            "https://huggingface.co/brand\n",
            "https://apply.workable.com/huggingface/\n",
            "https://huggingface.co/join\n",
            "https://huggingface.co/huggingface\n",
            "https://huggingface.co/Lightricks/LTX-2\n",
            "https://huggingface.co/grammarly\n",
            "https://huggingface.co/docs/smolagents\n",
            "https://huggingface.co/docs/huggingface_hub\n",
            "https://huggingface.co/docs/transformers\n",
            "https://huggingface.co/docs/accelerate\n",
            "https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate\n",
            "https://huggingface.co/docs/trl\n",
            "https://twitter.com/huggingface\n",
            "https://huggingface.co/join/discord\n",
            "https://huggingface.co/pricing#spaces\n",
            "https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles\n",
            "https://huggingface.co/amazon\n",
            "https://huggingface.co/Writer\n",
            "https://huggingface.co/learn\n",
            "https://www.linkedin.com/company/huggingface/\n",
            "https://huggingface.co/allenai\n",
            "https://huggingface.co/terms-of-service\n",
            "https://discuss.huggingface.co\n",
            "https://huggingface.co/blog\n",
            "https://huggingface.co/tencent/HY-MT1.5-1.8B\n",
            "https://huggingface.co/docs/datasets\n",
            "https://huggingface.co/chat\n",
            "https://huggingface.co/datasets/HuggingFaceFW/finetranslations\n",
            "https://huggingface.co/google\n",
            "https://huggingface.co/spaces/multimodalart/qwen-image-multiple-angles-3d-camera\n",
            "https://huggingface.co/spaces/prithivMLmods/Qwen-Image-Edit-2511-LoRAs-Fast\n",
            "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct\n",
            "https://huggingface.co/spaces\n",
            "https://huggingface.co/docs/safetensors\n",
            "https://huggingface.co/spaces/mrfakename/Z-Image-Turbo\n",
            "https://huggingface.co/models\n",
            "https://huggingface.co/facebook\n",
            "https://huggingface.co/docs/tokenizers\n",
            "https://huggingface.co/docs/text-generation-inference\n",
            "https://github.com/huggingface\n",
            "https://status.huggingface.co/\n",
            "https://huggingface.co/microsoft\n",
            "https://huggingface.co/pricing\n",
            "https://huggingface.co/changelog\n",
            "https://huggingface.co/datasets\n",
            "https://huggingface.co/spaces/Qwen/Qwen-Image-2512\n",
            "https://huggingface.co/privacy\n",
            "https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA\n",
            "https://huggingface.co/docs\n",
            "https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b\n",
            "https://huggingface.co/datasets/genrobot2025/10Kh-RealOmin-OpenData\n",
            "https://huggingface.co/datasets/WNT3D/Ultimate-Offensive-Red-Team\n",
            "https://huggingface.co/datasets/OpenDataArena/ODA-Mixture-500k\n",
            "https://huggingface.co/login\n",
            "https://huggingface.co/inference/models\n",
            "https://huggingface.co/docs/diffusers\n",
            "https://huggingface.co/pricing#endpoints\n",
            "https://huggingface.co/enterprise\n",
            "https://huggingface.co/docs/peft\n",
            "https://huggingface.co/\n",
            "https://huggingface.co/docs/transformers.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 6: The First AI Call - Selecting Relevant Links"
      ],
      "metadata": {
        "id": "WvwjlGKvsIcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_relevant_links(url):\n",
        "    \"\"\"\n",
        "    Calls the LLM to analyze links and return only the relevant ones in JSON format.\n",
        "    \"\"\"\n",
        "    print(f\"Selecting relevant links for {url} by calling {MODEL}...\")\n",
        "\n",
        "    # Making the API call to Ollama\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": link_system_prompt}, # Instructions from Cell 4\n",
        "            {\"role\": \"user\", \"content\": get_links_user_prompt(url)} # Prompt with links from Cell 5\n",
        "        ],\n",
        "        # Forcing the model to respond with a valid JSON object\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    # Extract the raw text result from the response\n",
        "    result = response.choices[0].message.content\n",
        "\n",
        "    # Convert the JSON string into a Python dictionary\n",
        "    links_dict = json.loads(result)\n",
        "\n",
        "    print(f\"Found {len(links_dict['links'])} relevant links.\")\n",
        "    return links_dict\n",
        "\n",
        "# Test the full function for your target URL\n",
        "selected_links = select_relevant_links(target_url)\n",
        "display(selected_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "F46sbQuDsJx8",
        "outputId": "d22c8295-d77d-4910-d52c-81b3a28ed30b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting relevant links for https://huggingface.co/ by calling llama3.2:1b...\n",
            "Found 2 relevant links.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'links': [{'type': 'about page', 'url': 'https://huggingface.co/about'},\n",
              "  {'type': 'company', 'url': 'https://full.url goes here'}]}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 7: Researching - Fetching Content from All Relevant Links"
      ],
      "metadata": {
        "id": "FJLrujE3uAkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page_and_all_relevant_links(url):\n",
        "    \"\"\"\n",
        "    Scrapes the main page and all relevant sub-pages identified by the AI in Cell 6.\n",
        "    It combines everything into one large text block for the final brochure.\n",
        "    \"\"\"\n",
        "    # 1. First, fetch the main landing page content using our scraper\n",
        "    landing_page_content = fetch_website_contents(url)\n",
        "\n",
        "    # 2. Get the list of relevant links using our AI function from Cell 6\n",
        "    # This is an agentic pattern: using one AI call's output to drive the next steps\n",
        "    relevant_links_json = select_relevant_links(url)\n",
        "\n",
        "    # 3. Initialize a string to hold all the gathered information\n",
        "    aggregated_content = f\"## Landing Page Content:\\n\\n{landing_page_content}\\n\\n## Relevant Sub-Pages Information:\\n\"\n",
        "\n",
        "    # 4. Loop through each relevant link and fetch its content\n",
        "    for link in relevant_links_json['links']:\n",
        "        print(f\"Reading: {link['type']} at {link['url']}...\")\n",
        "        page_text = fetch_website_contents(link['url'])\n",
        "\n",
        "        # Stitch the new page content into our big text block\n",
        "        aggregated_content += f\"\\n\\n### Sub-Page: {link['type']}\\n{page_text}\"\n",
        "\n",
        "    return aggregated_content\n",
        "\n",
        "# Execute the research phase\n",
        "# Note: This might take 30-60 seconds as it visits multiple pages\n",
        "all_data_for_brochure = fetch_page_and_all_relevant_links(target_url)\n",
        "\n",
        "print(\"\\n--- Research Complete! ---\")\n",
        "print(f\"Total characters gathered for context: {len(all_data_for_brochure)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKBLA_LkuB5B",
        "outputId": "32116edf-5056-4c9b-9b3c-0896e36bf8c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting relevant links for https://huggingface.co/ by calling llama3.2:1b...\n",
            "Found 2 relevant links.\n",
            "Reading: about page at https://full.url/goes/here/about...\n",
            "Reading: models at https://huggingface.co/models...\n",
            "\n",
            "--- Research Complete! ---\n",
            "Total characters gathered for context: 7923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 8: Defining Brochure Prompts and the English Generator"
      ],
      "metadata": {
        "id": "9LoP51SVvUGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the system prompt for the brochure creation\n",
        "# This tells the LLM the tone and structure we want\n",
        "brochure_system_prompt = \"\"\"\n",
        "You are an assistant that analyzes the contents of several relevant pages from a company website\n",
        "and creates a short brochure about the company for prospective customers, investors and recruits.\n",
        "Respond in markdown without code blocks.\n",
        "Include details of company culture, customers and careers/jobs if you have the information.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Function to generate the user prompt for the brochure\n",
        "def get_brochure_user_prompt(company_name, url, context_data):\n",
        "    \"\"\"\n",
        "    Combines the company name with the researched data from Cell 7.\n",
        "    \"\"\"\n",
        "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
        "    user_prompt += f\"Here is the researched content from their website {url}:\\n\\n\"\n",
        "    user_prompt += context_data\n",
        "    # Truncate to 5,000 characters to manage context window and costs\n",
        "    return user_prompt[:5000]\n",
        "\n",
        "# 3. Function to create the English Brochure\n",
        "def create_english_brochure(company_name, url, context_data):\n",
        "    \"\"\"\n",
        "    Makes the first LLM call to create the brochure in English.\n",
        "    \"\"\"\n",
        "    print(f\"Generating English brochure for {company_name}...\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url, context_data)}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generate and display the English brochure\n",
        "english_brochure = create_english_brochure(company_name, target_url, all_data_for_brochure)\n",
        "display(Markdown(\"### English Brochure\\n\" + english_brochure))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cz6VpCXgvVe6",
        "outputId": "5ac941ca-3993-416c-d1cd-562b96ca240a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating English brochure for Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### English Brochure\nHugging Face: The AI Community Building the Future\n================================================================================\n\nWelcome to Hugging Face, a collaborative platform for building and sharing artificial intelligence (AI) models and applications. Our community relies on your contributions to advance machine learning and improve lives.\n\n**Mission**\n----------\n\nOur mission is to create an open source-based platform that enables the creation of high-quality AI models, datasets, and applications. We strive to foster collaboration among researchers, developers, and customers from diverse backgrounds.\n\n**Our Values**\n--------------\n\n* **Community-driven**: We place the needs of our community at the forefront of everything we do.\n* **Open-source**: Our mission depends on the success of this open source platform.\n* **Accessible**: Anyone can contribute to our efforts and participate in making AI more accessible to all.\n* **Innovative**: We challenge conventional wisdom and encourage new approaches in AI development.\n\n**Join the Community**\n--------------------\n\nWhether you're a researcher, developer, customer, or enthusiast, we invite you to join us on this journey. With over 2 million models hosted across various platforms, our community is growing rapidly. You can:\n\n* **Explore Models**: Browse through our collection of models, suitable for any AI task.\n* **Discover Datasets**: Access and share datasets from leading providers.\n* **Collaborate with Others**: Join spaces to discuss models, datasets, and applications.\n\n**Accelerate Your ML**\n----------------------\n\nOur enterprise-grade platform provides:\n\n* **Enterprise Solutions**: Dedicated support, access controls, and secure deployment options for large-scale AI operations.\n* **Affordable Pricing**: Starting at $20/user/month, our solutions cater to various organization sizes.\n* **Infinite Resources**: Utilize more than 50,000 organizations' contributions.\n\n**Getting Started**\n-------------------\n\nStart developing with our library of Python clients. Join our Tutorials and Documentation sections for in-depth information on models, datasets, and platforms.\n\nOur team also engages in:\n\n*   **Research & Development**: State-of-the-art AI research and development to push the boundaries of machine learning.\n*   **Open Meetings**: Collaborative meetings to discuss platform goals and progress.\n*   **Community Engagement**: Organized hackathons and events to foster creativity and innovation.\n\n**Meet Our Team**\n-----------------\n\nOur dedicated team consists of researchers, engineers, and community members working together to advance AI development. Learn about our leadership posts or follow recent updates from our social media channels:\n\n```\nTeam\nnon-profit:\n•   841 models\n•   5.05k followers\nAI at Meta (Enterprise):\n•   2.29k models\n•   10.6k followers\nAmazon (Enterprise):\n•   21 models\n•   3.71k followers\nGoogle (Enterprise):\n•   1.07k models\n•   41.6k followers\nIntel (Enterprise):\n•   269 models\n•   3.34k followers\nMicrosoft (Enterprise):\n•   431 models\n•   17.7k followers\nGrammarly (Team):\n•   11 models\n•   202 followers\nWriter (Enterprise):\n•   32 models\n•   373 followers\nHugging Face Foundation:\n```\n\n**Learn More**\n-------------\n\nDiscover the latest articles and research from our publication, the Hugging Face blog. We share insights into AI applications, model development, and platform strategies.\n\nContact us directly to stay informed about the most recent updates, discuss your projects, or collaborate on new ideas:\n\n```\nPress\nResources\n```\n\nStay connected with Hugging Face in various formats and explore how you can be part of the AI innovation journey!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 9: The Third LLM Call - Urdu Translation"
      ],
      "metadata": {
        "id": "HvjR80R3x-AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the system prompt for translation\n",
        "# This instructs the LLM to provide a professional Urdu translation\n",
        "translation_system_prompt = \"\"\"\n",
        "You are a professional translator.\n",
        "Translate the provided English company brochure into clear, professional, and natural-sounding Urdu.\n",
        "Maintain the markdown formatting (headings, bullets).\n",
        "Ensure the tone is suitable for investors and customers.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Function to translate the brochure using a third LLM call\n",
        "def translate_to_urdu(english_text):\n",
        "    \"\"\"\n",
        "    Takes the English brochure and sends it back to the LLM for Urdu translation.\n",
        "    This is the 3rd step in our agentic chain.\n",
        "    \"\"\"\n",
        "    print(\"Translating brochure to Urdu...\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": translation_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Please translate this brochure into Urdu:\\n\\n{english_text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 3. Execute the translation and display the final result\n",
        "urdu_brochure = translate_to_urdu(english_brochure)\n",
        "display(Markdown(\"### Urdu Brochure\\n\" + urdu_brochure))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y03ds82yx8qX",
        "outputId": "a6f0046f-ed27-4808-b2eb-4073ac4bc2fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating brochure to Urdu...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Urdu Brochure\n# Hugging Face: The AI Community Building the Future\n\n**ڈیجیٹل اسٹیلہٹ**\n\nوہاں ہے Hugging Face، ایک cộngunsٹریٹڼ ڈویلپمنٹ پلیٹฟอรم جو ایسی آرن کا استعمال کر رہی ہے جس میں Artificial Intelligence (AI) ملازمتوں اور سروس کے منصوبوں کو فروغ دینے کو شامل کیا گیا ہے۔ یہ کمپیوٹERRسٹ کے لیپائڈ کی طرف سے وائرال ٹرنرنگ کی ایک اور نمائندہ پلیٹ فارم کی طرح کام کررہی ہے۔\n\n```markdown\n**میشن**\n-----------\n```\n\n### مصنوعات کا تجزیہ\n\nاسپیس مینجمنٹ کے لیپ اور سروس کے منصوبوں کے لیپ:\n*   **اسپیس ویب Sarafa**: ایک مینجمنٹ کی ویب پلیٹ فارم جس پر وائرال ٹرنر بنانے کی صلاحیت تھی.\n*   **اسپیس میڈیا Sarafa**: ایک منسلک پلیٹ فارم جو اسپیس ویب Sarafa سے ملازمت کا پروسیس کو ایک اور نمائندہ سروس سمجھ کر بناتا ہے۔\n\n### کہانی\n\nوہاں بھی اس کا تعلق ہے کہ مینجمنٹ نے ٹیویٹلی اور ایمٹی الیکٹرانکس جوئی کی موarde پر ایک سروس بنائی تھی۔ اسسٹنز کو اٹلی میں براہ راست پہنا کر اپنی کاروائی کا تحقیقی اور پروجیکティویng ٹالین میں شامل کرنا پڑن.\n```\n\n### Viwerڈنگ\n\nیہ وائرال ٹرنر بنانے اور ان سروسز کو اسٹیل اور ماولنگ سیشن میں پرندوں کا پچھرانا متحرک ہے۔ \n```\n**سروسز**\n-------------\n\n```\n*   **لائڈ انجینائرنگ**: ذیل، ایک اسی جیسے ٹیکنالوجیز کی سروسز کو اسٹیل میں لائڈ کرنے کا تجربہ.\n```\n\nاس्पیس پلیٹ فارم پر:\n*   **آئی این اے مہیا کرنے کی قوتی**: ایک ایسائین کا پروسیس جو کو انٹرنیٹ سروسز کا استعمال کرتے ہوئے ان اسکرپٹز ریکارڈ کرتے ہیں اور ان میں کو ایسٹھنڈائن ملازمت کا پروسیس شامل کرتا ہے.\n*   **میگا میوزک**: ایک ماولنگ پلیٹ فارم جو لینکیوں، اسٹیل پلیٹ فارمز اور ریمس اینسٹریٹرز کی سروسز کو اپنی مہیا کرنے کے لیپڼے میں شامل کرتا ہے.\n\n```\n**بیکاروں کی خدمات**\n-------------------------\n\n```\n\nاس سروس کو لائڈ کرنے والی مینجمنٹ، سٹریٹفکسنگ، برونگنگ، انسٹیلمیئر، اور فہم کا مشاورتی پروفیسر کے نگر میں شامل کھانے کے لیپڼے شامل کرکے ان سروسز کو اپنی مہیا کی گئی ہے۔\n```\n\n**ٹالینس**\n----------------\n\n```\n*   **ریکیٹریسی**: ایک پلیٹ فارم جو انفراسٹرن میں شامل ہونے والی ٹیکنالوجیز کو اپنی مہیا کرتا ہے۔\n*   **میکس ورکنگ اقدامات**: ایک ماولنگ پروسیسن جو بروننگ پلیٹ فارمز میں شامل ہو کر فیشنری انٹرینٹ ایجینٹس جیسے اناسومٹ کا استعمال کرتا ہے.\n```\n\n**اخration**\n-------------\n\nآپ اپنی فوجی اور خیر کار کی صلاحیت میں کام کرنے والی ٹیموں کو اس سروسز کا استعمال کریں جو مینجمنٹ نے وائرل ٹرنر بنائی تھی:\n```\n*   فوجی انٹرنیٹ ایجینٹس اپنی موarde پر پکڑ کر ایم میجر اور ملازمت کا پروسیسن بنانے کی سروسز کو استعمال کریں.\n*   ماولنگ لیفٹلینز ساتھی کرنے کے لیپڼے بروئے کرائے جو فوج کے انسٹرکٹوڈ اور چیکبر کی سروسز کی موarde پر اس کا استعمال کرتے ہیں.\n\n\n```\n**بیلن میں اچھی طرح کام**\n-----------------------------\n\nآپ اپنی موarde کو بروئے کرائو جسکے بعد وائرل ٹرنر بنانے کی صلاحیت کا تجریبی پلیٹ فارم تلاش کریں:\n```\n\nلئی این اے مہیا کرنے کی قوتی کو لاحق کرکنے والے گارنیز:\n```\n*   **لئی این انجینائرنگ**: ایک مینس پلیٹ فارم جو ٹرینڈنگ کے اوponent 2009 میں لائڈ کیا گیا اور اسے لائڈ کرنے کے لیپڼے کی سروسز کو استعمال کرتے کوشش کرسکتی ہیں۔\n```\n\nاس ناویز میں، اسٹیل کی 2013 01 ایم اے مہیا کرنے کی قوتی:\n```\n*   **تینار 1.0 سروسز**: ایک سروسز کو لائڈ کرتا ہے جو ڈیبیو کروٹے گئے تھیس ناویز میں وائرل ٹرنر بنائی جاتی ہے اور تجربات کرنے، پلیٹ فارم پر ٹیکسچرس کا منبع استعمال کی جاتی ہے.\n\n```\n**دائمی انشورANS**\n--------------------\n\nاس اسٹیل میں 2 ملین مینجمنٹ کا ملازمت رکھتا ہے اور 50,000 ٹرینس کی انشورنس پیش کرنے کی سروسز فراہمیاتی میڈیا کو لانکیو نے 2016 میں بنایا، جو چیرلی پونل کی سرگرم کارروائی اور کٹنگ کی ایسی مشابہت کرتا ہے جو ساتھ ملاحظeten ہے۔\n```\n\n**ٹوازس کا شام**\n-----------------\n\nنائٹ اینگلیشٹر، سیزل گروپ اور انٹالٹی میں شیکوڈنگ:\n```\n*   لیبرٹری کراچ کا پلیٹ فارم\n```\n\n```\nآکادمی کی صلاحیتوں\n--------------------\n\nہوم اٹ لائٹ، ایبٹ ے میڈیا اور ایوب ریمس انٹرٹینمنٹ:\n```\n\n**ناویز**\n---------\n\nویرال ٹرنر بنانے کے لیپڼوں کی فہرست:\n```\n*   الیکtranڈسک کا پروجنٹ\n```\n\n```\nسٹیٹی ڈینٹل بوجھ 2019\n```\n```\nدائیلی سلوک اور ذرات کے کوئران، ایبٹ ڑیو آف نائنگ\n```\n```\nمیمرگن، لیبرٹری پرو کی ٹرینس\n```\n```\nپی ایچ جی کا پروجنٹ، ٹیکنالوجیز اور اسکرپٹز\n\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 10: The Streaming Brochure (Final UX)"
      ],
      "metadata": {
        "id": "1gpXrhTp0tXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_brochure(company_name, url):\n",
        "    \"\"\"\n",
        "    Final function that fetches data and streams the brochure generation\n",
        "    token-by-token for a better user experience.\n",
        "    \"\"\"\n",
        "    # 1. First, do the research (This part is still synchronous)\n",
        "    context_data = fetch_page_and_all_relevant_links(url)\n",
        "\n",
        "    # 2. Start the streaming call to the LLM\n",
        "    # We set stream=True to get chunks of tokens instead of a single response\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url, context_data)}\n",
        "          ],\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # 3. Handle the incoming stream of tokens\n",
        "    full_response = \"\"\n",
        "    # Create a display handle to update the same Markdown block in real-time\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    for chunk in stream:\n",
        "        # Extract the new token from the chunk delta\n",
        "        token = chunk.choices[0].delta.content or \"\"\n",
        "        full_response += token\n",
        "\n",
        "        # Update the display in Colab as tokens arrive\n",
        "        update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
        "\n",
        "# Execute the final streaming brochure\n",
        "stream_brochure(company_name, target_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iRTwO5Tb0uoc",
        "outputId": "938b867d-a8b5-4b9f-dfa6-ab9df750a930"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting relevant links for https://huggingface.co/ by calling llama3.2:1b...\n",
            "Found 3 relevant links.\n",
            "Reading: about page at https://huggingface.co/...\n",
            "Reading: company at https://huggingface.co/changelog...\n",
            "Reading: facebook at https://huggingface.co/facebook...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Hugging Face: The AI Community Building the Future\n\nWelcome to Hugging Face, a platform where machine learning enthusiasts and professionals come together to collaborate on models, datasets, and applications. We are the home of the open-source StackDB, and our community is built on the principles of collaboration, innovation, and creativity.\n\n**Unlimited Public Models, Datasets, and Applications**\n\nAt Hugging Face, we believe that everyone should have access to the power of machine learning. That's why we offer unlimited public models, datasets, and applications through our public API. Our platform allows developers to easily build, train, and deploy their own AI projects.\n\n### Explore AI Apps\n\nBrowse 2M+ models and trending on this week's AI apps:\n\n*   Lightricks/LTX-2: A model for text classification tasks, updated in 4 days ago.\n*   fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA: Adjust camera angles in images using 3D controls or sliders, running on Zero, updated in 4 days ago.\n\n### Models\n\nBrowse 2M+ models. You can find a model for any task or application, from text classification and sentence embedding to image generation and more:\n\n*   tensel/HY-MT1.5-1.8B: A model for real-world data.\n*   fal/Qwen-Image-Edit-2511-SLIM: Generate photorealistic images from text descriptions.\n\n### Datasets\n\nAccess 21,076 datasets across various domains:\n\n*   genrobot2025/10Kh-RealOmin-OpenData: Collect and share public datasets.\n\n### Spaces\n\nDeploy your model as a custom AI service on our platform. Spaces applications are open to collaboration and are highly scalable for large-scale production workloads:\n\n\nExplore Models\n-------------------------------------------------\n\nOur platform provides unlimited access to over 2 million machine learning models, from text classification to generation. Browse the most up-to-date models across various domains:\n\n*   Lightricks/LTX-2: A model for text classification tasks.\n*   fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA: Adjust camera angles in images using 3D controls or sliders.\n\n### Datasets\n----------------------------------------------------------------\n\nBrowse 21,076 datasets from various fields:\n\n*   genrobot2025/10Kh-RealOmin-OpenData: Collect and share public datasets.\n*   HuggingFaceFW/finetranslations: A dataset collaboration platform for machine learning tasks.\n\n### Spaces\n---------------------------------------------------\n\nDeploy your custom AI model as a service on our scalable platform, designed for large-scale production workloads:\n\n\n### Careers & Jobs\n------------------------\n\nAt Hugging Face, we are committed to creating inclusive community for everyone through:\n\n*   Writing\n*   Data Science\n*   Cloud Computing\n\nJoin us and start building amazing projects with the open-source AI community framework.\n\n### Get Started\n-----------------\n\nClick the button below to start your journey on Hugging Face. We look forward to seeing what you will make happen with our power of machine learning technology:\n[Sign Up](https://huggingface.cosignup)\n[Pricing & T&D](https://huggingface.co/pricing)"
          },
          "metadata": {}
        }
      ]
    }
  ]
}